# -*- coding: utf-8 -*-
"""FINAL PROJECT PSI TFT temperature forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CKTUjvBI-kwRSlnH_JChIoDpwXLo5r3Q
"""

import pandas as pd
import numpy as np

"""# Install need packages"""

!pip install lightning

!pip install pytorch-forecasting

!pip install optuna-integration

"""# Load the dataset"""

data = pd.read_csv("/content/Portugal 2022-08-01 to 2023-08-01.csv")

data

"""# Import need libraries"""

import lightning.pytorch as pl
from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor
from lightning.pytorch.loggers import TensorBoardLogger
import numpy as np
import pandas as pd
import torch

from pytorch_lightning import Trainer
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_forecasting.data import GroupNormalizer
from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss

from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet
from pytorch_forecasting.data import GroupNormalizer
from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss

"""# Prepare dataset"""

data.isnull().sum()

max_encoder_length = 330
max_prediction_length = 30

data["datetime"] = pd.to_datetime(data["datetime"])

data["time_idx"] = (data["datetime"] - data["datetime"].min()).dt.days

data["time_idx"]

training_cutoff = data["time_idx"].max() - max_prediction_length

data["group_id"] = 0

data["month"] = data["datetime"].dt.month
data["dayofweek"] = data["datetime"].dt.dayofweek

"""# Division of the dataset into training, validation, and test sets"""

training = TimeSeriesDataSet(
    data[lambda x: x.time_idx <= training_cutoff],
    time_idx="time_idx",
    target="temp",
    group_ids=["group_id"],
    min_encoder_length=30,
    max_encoder_length=max_encoder_length,
    min_prediction_length=7,
    max_prediction_length=max_prediction_length,
    static_categoricals=["name"],
    static_reals=[],
    time_varying_known_categoricals=[],
    variable_groups={},
    time_varying_known_reals=["solarenergy","tempmax", "tempmin", "uvindex", "severerisk", "moonphase", "month", "dayofweek"],
    time_varying_unknown_categoricals=[],
    time_varying_unknown_reals=["temp"],
    target_normalizer=GroupNormalizer(
        groups=["group_id"], transformation="softplus"
    ),
    add_relative_time_idx=True,
    add_target_scales=True,
    add_encoder_length=True,
    allow_missing_timesteps=True
)

validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)

batch_size = 64
train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)
val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)

baseline_predictions = Baseline().predict(val_dataloader, return_y=True)
MAE()(baseline_predictions.output, baseline_predictions.y)

test_cutoff = data["time_idx"].max() - max_prediction_length

test_cutoff = test_cutoff - (training.min_encoder_length + training.min_prediction_length)

test = TimeSeriesDataSet.from_dataset(
    training,
    data[lambda x: x.time_idx > test_cutoff],
    predict=True,
    stop_randomization=True
)

test_dataloader = test.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)

"""# Hyperparameter optimization"""

early_stop_callback = EarlyStopping(monitor="val_loss", patience=10, verbose=True, mode="min")
lr_monitor = LearningRateMonitor(logging_interval='step')

import optuna
from optuna.integration import PyTorchLightningPruningCallback

def objective(trial):
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)
    hidden_size = trial.suggest_int('hidden_size', 8, 128)
    attention_head_size = trial.suggest_int('attention_head_size', 1, 4)
    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    hidden_continuous_size = trial.suggest_int('hidden_continuous_size', 8, 128)

    tft = TemporalFusionTransformer.from_dataset(
        training,
        learning_rate=learning_rate,
        hidden_size=hidden_size,
        attention_head_size=attention_head_size,
        dropout=dropout,
        hidden_continuous_size=hidden_continuous_size,
        loss=QuantileLoss(),
        optimizer="Adam"
    )

    trainer = pl.Trainer(
        max_epochs=10,
        accelerator="cpu",
        gradient_clip_val=0.1,
        log_every_n_steps=1,
        callbacks=[
            PyTorchLightningPruningCallback(trial, monitor="val_loss"),
            early_stop_callback,
            lr_monitor
        ],
        logger=TensorBoardLogger("lightning_logs")
    )

    trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)
    val_loss = trainer.callback_metrics["val_loss"].item()
    return val_loss

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=20)

print(f"Najlepszy trial: {study.best_trial.value}")
print(f"Najlepsze parametry: {study.best_trial.params}")

"""# Training the model over multiple epochs"""

pl.seed_everything(42)
trainer = pl.Trainer(
    max_epochs=15,
    accelerator="cpu",
    gradient_clip_val=0.1,
     log_every_n_steps=1
)

tft = TemporalFusionTransformer.from_dataset(
    training,
    learning_rate=0.0010050756021825961,
    hidden_size=116,
    attention_head_size=3,
    dropout=0.49960234041899343,
    hidden_continuous_size=119,
    loss=QuantileLoss(),
    optimizer="Adam"
)

trainer.fit(
    tft,
    train_dataloaders=train_dataloader,
    val_dataloaders=val_dataloader,
)

"""# Model Evaluation Metrics Comparison"""

from sklearn.metrics import mean_squared_error

predictions = tft.predict(val_dataloader)

actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])

mae = MAE()(predictions, actuals)

mae

mse = mean_squared_error(predictions, actuals)
mse

rmse = np.sqrt(mse)
rmse

"""# Visualization of Forecast Results and Model Interpretation"""

raw_predictions = tft.predict(val_dataloader, mode="raw", return_x=True)

tft.hparams.max_encoder_length = 400

for idx in range(1):
    tft.plot_prediction(raw_predictions.x, raw_predictions.output, idx=idx, add_loss_to_title=True)

interpretation = tft.interpret_output(raw_predictions.output, reduction="sum")
tft.plot_interpretation(interpretation)

test_predictions = tft.predict(test_dataloader)

test_actuals = torch.cat([y[0] for x, y in iter(test_dataloader)])
test_mae = MAE()(test_predictions, test_actuals)
print(f"MAE dla zbioru testowego: {test_mae}")

test_mse = mean_squared_error(test_predictions, test_actuals)
test_rmse = np.sqrt(test_mse)
print(f"MSE dla zbioru testowego: {test_mse}")
print(f"RMSE dla zbioru testowego: {test_rmse}")

raw_test_predictions = tft.predict(test_dataloader, mode="raw", return_x=True)

for idx in range(1):
    tft.plot_prediction(raw_test_predictions.x, raw_test_predictions.output, idx=idx, add_loss_to_title=True)

test_interpretation = tft.interpret_output(raw_test_predictions.output, reduction="sum")
tft.plot_interpretation(test_interpretation)

"""# Model Saving and Loading"""

trainer.save_checkpoint("/content/drive/MyDrive/tft_checkpoint.ckpt")

from pytorch_forecasting import TemporalFusionTransformer

tft_loaded = TemporalFusionTransformer.load_from_checkpoint(
    "/content/drive/MyDrive/tft_checkpoint.ckpt"
)

new_data = data[lambda x: x.time_idx > training_cutoff]
new_dataset = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)
new_dataloader = new_dataset.to_dataloader(train=False, batch_size=64, num_workers=0)

raw_predictions = tft_loaded.predict(new_dataloader, mode="raw", return_x=True)
tft_loaded.plot_prediction(raw_predictions.x, raw_predictions.output, idx=0)